{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3933a57d-2d72-4cfe-a70b-8a72192db412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "from psutil import users\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69c58a3b-eba0-43fc-af80-58fc5a4d92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Data preprocessing module for job recommendation system.\n",
    "\n",
    "This module handles the preprocessing of user and job data for the recommendation system,\n",
    "including text cleaning, skill parsing, and embedding generation for skills and titles.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing job and user data.\n",
    "\n",
    "    Handles the cleaning, parsing, and vectorization of job and user data\n",
    "    for use in recommendation algorithms.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    stop_words : set\n",
    "        Set of English stop words from NLTK\n",
    "    lemmatizer : WordNetLemmatizer\n",
    "        NLTK lemmatizer for text preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with stop words and lemmatizer.\n",
    "\n",
    "        Loads stopwords from a local pickle file if available,\n",
    "        otherwise downloads them from NLTK and saves for future use.\n",
    "        \"\"\"\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        try:\n",
    "            import os\n",
    "            stopwords_file = 'nltk_stopwords.pkl'\n",
    "            if os.path.exists(stopwords_file):\n",
    "                with open(stopwords_file, 'rb') as f:\n",
    "                    self.stop_words = pickle.load(f)\n",
    "                print(\"Loaded stopwords from local file\")\n",
    "            else:\n",
    "                try:\n",
    "                    self.stop_words = set(stopwords.words('english'))\n",
    "                except LookupError:\n",
    "                    nltk.download('stopwords')\n",
    "                    self.stop_words = set(stopwords.words('english'))\n",
    "                with open(stopwords_file, 'wb') as f:\n",
    "                    pickle.dump(self.stop_words, f)\n",
    "                print(\"Downloaded stopwords and saved to local file\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling stopwords file: {e}\")\n",
    "            try:\n",
    "                self.stop_words = set(stopwords.words('english'))\n",
    "            except LookupError:\n",
    "                nltk.download('stopwords')\n",
    "                self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def get_user_skills(self, skill_dicts):\n",
    "        \"\"\"\n",
    "        Extract skill names from a list of skill dictionaries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        skill_dicts : list\n",
    "            List of dictionaries containing skill information\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of skill names, or empty list if input is None or invalid\n",
    "        \"\"\"\n",
    "        if not skill_dicts:\n",
    "            return []\n",
    "        skills = []\n",
    "        for skill_dict in skill_dicts:\n",
    "            if isinstance(skill_dict, dict) and 'name' in skill_dict and isinstance(skill_dict['name'], str):\n",
    "                skills.append(skill_dict['name'])\n",
    "        return skills\n",
    "\n",
    "    def parse_skills(self, skills_list):\n",
    "        \"\"\"\n",
    "        Parse a list of skill strings into a list of skill phrases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        skills_list : list\n",
    "            List of strings containing concatenated skills\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of parsed skill phrases, or empty list if input is None or invalid\n",
    "        \"\"\"\n",
    "        def clean_string(skill_list):\n",
    "            if not skill_list:\n",
    "                return []\n",
    "            clean_list = []\n",
    "            for s in skill_list:\n",
    "                if isinstance(s, str):\n",
    "                    clean_list.append(re.sub(r'\\s*\\(.*?\\)', '', s))\n",
    "            return clean_list\n",
    "\n",
    "        def standardize_skill(skill):\n",
    "            if not isinstance(skill, str):\n",
    "                return \"\"\n",
    "            skill = skill.lower()\n",
    "            skill = re.sub(r'[^\\w\\s]', '', skill)\n",
    "            return skill.strip()\n",
    "\n",
    "        def parse_skill_list(skills_list):\n",
    "            skills = []\n",
    "            for skills_string in skills_list:\n",
    "                if not isinstance(skills_string, str):\n",
    "                    continue\n",
    "                words = skills_string.split()\n",
    "                words = [word for word in words if word not in self.stop_words]\n",
    "\n",
    "                current_skill = []\n",
    "                prev_starts_with_capital = False\n",
    "\n",
    "                for word in words:\n",
    "                    if word:\n",
    "                        starts_with_capital = word[0].isupper()\n",
    "                        if starts_with_capital:\n",
    "                            skill_phrase = ' '.join(current_skill)\n",
    "                            if skill_phrase:\n",
    "                                std_skill_phrase = standardize_skill(skill_phrase)\n",
    "                                if std_skill_phrase:\n",
    "                                    skills.append(std_skill_phrase)\n",
    "                            current_skill = [word]\n",
    "                        else:\n",
    "                            current_skill.append(word)\n",
    "                        prev_starts_with_capital = starts_with_capital\n",
    "\n",
    "                if current_skill:\n",
    "                    skill_phrase = ' '.join(current_skill)\n",
    "                    if skill_phrase:\n",
    "                        std_skill_phrase = standardize_skill(skill_phrase)\n",
    "                        if std_skill_phrase:\n",
    "                            skills.append(std_skill_phrase)\n",
    "\n",
    "            return skills\n",
    "\n",
    "        cleaned = clean_string(skills_list)\n",
    "        parsed_skills = parse_skill_list(cleaned)\n",
    "        return parsed_skills\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by removing special characters, lowercasing, and lemmatizing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str, list, or dict\n",
    "            Text to be preprocessed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str, list, or dict\n",
    "            Preprocessed text, or empty equivalent if input is None or invalid\n",
    "        \"\"\"\n",
    "        def preprocess_string(text_str):\n",
    "            if not isinstance(text_str, str):\n",
    "                return \"\"\n",
    "            text_str = re.sub(r\"[^a-zA-Z0-9 _-]\", \"\", text_str)\n",
    "            text_str = text_str.lower().strip()\n",
    "            words = word_tokenize(text_str)\n",
    "            filtered_words = [self.lemmatizer.lemmatize(word) for word in words if word.lower() not in self.stop_words]\n",
    "            return \" \".join(filtered_words)\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            return preprocess_string(text)\n",
    "        elif isinstance(text, list):\n",
    "            return [self.preprocess_text(item) for item in text]\n",
    "        elif isinstance(text, dict):\n",
    "            return {self.preprocess_text(key) if isinstance(key, str) else key: self.preprocess_text(value) if isinstance(value, str) else value\n",
    "                    for key, value in text.items()}\n",
    "        return \"\"\n",
    "\n",
    "    def get_last_experience(self, experience_dict):\n",
    "        \"\"\"\n",
    "        Get the most recent experience from user data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        experience_dict : list\n",
    "            List of experience dictionaries\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The most recent experience, or None if no valid experiences exist\n",
    "        \"\"\"\n",
    "        if not experience_dict or not isinstance(experience_dict, list):\n",
    "            return None\n",
    "        start_date = \"\"\n",
    "        last_exp = None\n",
    "        for job in experience_dict:\n",
    "            if isinstance(job, dict) and 'start_date' in job and isinstance(job['start_date'], str):\n",
    "                if job['start_date'] > start_date:\n",
    "                    start_date = job['start_date']\n",
    "                    last_exp = job\n",
    "        return last_exp\n",
    "\n",
    "    def preprocess_object(self, users_df, job_df):\n",
    "        \"\"\"\n",
    "        Preprocess object columns in DataFrames.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        users_df : pandas.DataFrame\n",
    "            User data DataFrame\n",
    "        job_df : pandas.DataFrame\n",
    "            Job data DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (preprocessed_users_df, preprocessed_job_df)\n",
    "        \"\"\"\n",
    "        if not job_df.empty:\n",
    "            for col in job_df.columns:\n",
    "                if job_df[col].dtype == 'object' and col != 'skills':\n",
    "                    job_df[col] = job_df[col].apply(self.preprocess_text)\n",
    "\n",
    "        if not users_df.empty:\n",
    "            for col in users_df.columns:\n",
    "                if users_df[col].dtype == 'object' and col != 'skills':\n",
    "                    users_df[col] = users_df[col].apply(self.preprocess_text)\n",
    "\n",
    "            users_df['last_experience'] = users_df['experiences'].apply(self.get_last_experience)\n",
    "            users_df['location_type'] = users_df['last_experience'].apply(\n",
    "                lambda x: x['location_type'] if x and isinstance(x, dict) and 'location_type' in x else ''\n",
    "            )\n",
    "            users_df['employment_type'] = users_df['last_experience'].apply(\n",
    "                lambda x: x['employment_type'] if x and isinstance(x, dict) and 'employment_type' in x else ''\n",
    "            )\n",
    "\n",
    "        return users_df, job_df\n",
    "\n",
    "    def preprocess_user_skills(self, skills):\n",
    "        \"\"\"\n",
    "        Extract skill values from user skills data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        skills : list\n",
    "            List of skill dictionaries\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of skill values, or empty list if input is None or invalid\n",
    "        \"\"\"\n",
    "        if not skills:\n",
    "            return []\n",
    "        return [value for d in skills for value in d.values() if isinstance(value, str)]\n",
    "\n",
    "    def clean_and_tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Clean and tokenize text.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to be cleaned and tokenized\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of cleaned tokens, or empty list if input is None or invalid\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        return words\n",
    "\n",
    "    def get_top_10_words(self, description):\n",
    "        \"\"\"\n",
    "        Get the top 10 most common words in a description.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        description : str\n",
    "            Text description\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of top 10 words, or empty list if input is None or invalid\n",
    "        \"\"\"\n",
    "        if not isinstance(description, str):\n",
    "            return []\n",
    "        words = self.clean_and_tokenize(description)\n",
    "        word_counts = Counter(words)\n",
    "        top_10 = word_counts.most_common(10)\n",
    "        return [word for word, count in top_10]\n",
    "\n",
    "    def precompute_skill_embeddings(self, users_df, job_df):\n",
    "        \"\"\"\n",
    "        Precompute embeddings for all skills in the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        users_df : pandas.DataFrame\n",
    "            User data DataFrame\n",
    "        job_df : pandas.DataFrame\n",
    "            Job data DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary mapping skills to embeddings\n",
    "        \"\"\"\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        all_skills = set()\n",
    "        if not job_df.empty:\n",
    "            for skills in job_df['skills']:\n",
    "                if skills:\n",
    "                    all_skills.update(skills)\n",
    "        if not users_df.empty:\n",
    "            for skills in users_df['skills']:\n",
    "                if skills:\n",
    "                    all_skills.update(skills)\n",
    "\n",
    "        all_skills = list(all_skills)\n",
    "        skill_embeddings = model.encode(all_skills, batch_size=128, show_progress_bar=True) if all_skills else {}\n",
    "        skill_to_embedding = dict(zip(all_skills, skill_embeddings)) if all_skills else {}\n",
    "        with open('skill_embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(skill_to_embedding, f)\n",
    "        return skill_to_embedding\n",
    "\n",
    "    def precompute_title_embeddings(self, users_df, job_df):\n",
    "        \"\"\"\n",
    "        Precompute embeddings for job titles and user subtitles.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        users_df : pandas.DataFrame\n",
    "            User data DataFrame\n",
    "        job_df : pandas.DataFrame\n",
    "            Job data DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary mapping titles/subtitles to embeddings\n",
    "        \"\"\"\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        all_titles = set()\n",
    "        if not job_df.empty:\n",
    "            for title in job_df['title']:\n",
    "                if isinstance(title, str) and title.strip():\n",
    "                    all_titles.add(title)\n",
    "        if not users_df.empty:\n",
    "            for subtitle in users_df['subtitle']:\n",
    "                if isinstance(subtitle, str) and subtitle.strip():\n",
    "                    all_titles.add(subtitle)\n",
    "\n",
    "        all_titles = list(all_titles)\n",
    "        title_embeddings = model.encode(all_titles, batch_size=128, show_progress_bar=True) if all_titles else {}\n",
    "        title_to_embedding = dict(zip(all_titles, title_embeddings)) if all_titles else {}\n",
    "        with open('title_embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(title_to_embedding, f)\n",
    "        return title_to_embedding\n",
    "\n",
    "    def preprocess(self, input_json='test.json'):\n",
    "        \"\"\"\n",
    "        Main preprocessing function to process input JSON data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_json : str or dict, optional\n",
    "            Path to input JSON file or dictionary, by default 'test.json'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (users_df, job_df, skill_embeddings, title_embeddings) Preprocessed DataFrames and embeddings\n",
    "        \"\"\"\n",
    "        if isinstance(input_json, str):\n",
    "            try:\n",
    "                with open(input_json, 'r') as f:\n",
    "                    input_data = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {input_json} not found\")\n",
    "                return pd.DataFrame(), pd.DataFrame(), {}, {}\n",
    "        else:\n",
    "            input_data = input_json\n",
    "\n",
    "        if not isinstance(input_data, dict):\n",
    "            print(\"Error: Input data must be a dictionary\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), {}, {}\n",
    "\n",
    "        # Handle 'job' or 'jobs' keys\n",
    "        job_data = input_data.get('job', input_data.get('jobs', []))\n",
    "        if isinstance(job_data, dict):\n",
    "            job_data = [job_data]\n",
    "\n",
    "        # Handle 'users' or 'user' keys\n",
    "        user_data = input_data.get('users', input_data.get('user', []))\n",
    "        if isinstance(user_data, dict):\n",
    "            user_data = [user_data]\n",
    "\n",
    "        # Filter users with valid about_me\n",
    "        valid_users = []\n",
    "        for user in user_data:\n",
    "            if not isinstance(user, dict):\n",
    "                continue\n",
    "            about_me = user.get('about_me', \"\")\n",
    "            if isinstance(about_me, str) and about_me.strip():\n",
    "                valid_users.append({\n",
    "                    'id': user.get('id', 0),\n",
    "                    'about_me': about_me,\n",
    "                    'subtitle': user.get('subtitle', \"\"),\n",
    "                    'skills': user.get('skills', []),\n",
    "                    'experiences': user.get('experiences', []),\n",
    "                    'educations': user.get('educations', [])\n",
    "                })\n",
    "\n",
    "        users_df = pd.DataFrame(valid_users)\n",
    "        job_df = pd.DataFrame(job_data)\n",
    "\n",
    "        if not users_df.empty:\n",
    "            users_df = users_df.drop_duplicates(subset=['about_me'], keep='first').reset_index(drop=True)\n",
    "            users_df = users_df.iloc[:300].copy()\n",
    "            users_df['skills'] = users_df['skills'].apply(self.get_user_skills)\n",
    "\n",
    "        if not job_df.empty:\n",
    "            job_df = job_df.drop_duplicates(subset=['description'], keep='first').reset_index(drop=True)\n",
    "            job_df['skills'] = job_df['skills'].apply(self.parse_skills)\n",
    "\n",
    "        users_df, job_df = self.preprocess_object(users_df, job_df)\n",
    "\n",
    "        if not job_df.empty:\n",
    "            job_df['top_10_words'] = job_df['description'].apply(self.get_top_10_words)\n",
    "        if not users_df.empty:\n",
    "            users_df['top_10_words'] = users_df['about_me'].apply(self.get_top_10_words)\n",
    "\n",
    "        skill_embeddings = self.precompute_skill_embeddings(users_df, job_df)\n",
    "        title_embeddings = self.precompute_title_embeddings(users_df, job_df)\n",
    "\n",
    "        if not job_df.empty:\n",
    "            job_df.to_pickle('job_df.pkl')\n",
    "        if not users_df.empty:\n",
    "            users_df.to_pickle('users_df.pkl')\n",
    "\n",
    "        return users_df, job_df, skill_embeddings, title_embeddings\n",
    "\n",
    "\n",
    "def preprocess(input_data):\n",
    "    \"\"\"\n",
    "    Preprocess the input JSON data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : str or dict\n",
    "        Path to input JSON file or dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (users_df, job_df, skill_embeddings, title_embeddings) Preprocessed DataFrames and embeddings\n",
    "    \"\"\"\n",
    "    preprocessor = DataPreprocessor()\n",
    "    return preprocessor.preprocess(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91c8f7af-b06f-4eae-9ea7-6ccbe24f0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Job-to-user recommendation module.\n",
    "\n",
    "This module provides functionality to recommend users for a job based on various\n",
    "similarity metrics including title similarity, description similarity, skill matching,\n",
    "employment type, and location type.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class UserRecommender:\n",
    "    \"\"\"\n",
    "    A class for recommending users for a job.\n",
    "\n",
    "    Provides methods to calculate various similarity metrics between a job and user profiles,\n",
    "    and combines these metrics to generate recommendations.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    title_weight : float\n",
    "        Weight for title similarity in the final score\n",
    "    cosine_weight : float\n",
    "        Weight for description cosine similarity in the final score\n",
    "    jaccard_weight : float\n",
    "        Weight for skill similarity in the final score\n",
    "    emp_type_weight : float\n",
    "        Weight for employment type matching in the final score\n",
    "    loc_type_weight : float\n",
    "        Weight for location type matching in the final score\n",
    "    top_n : int\n",
    "        Number of recommendations to return\n",
    "    title_embeddings : dict\n",
    "        Precomputed title and subtitle embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, title_embeddings, title_w=0.2, cosine_w=0.2, jaccard_w=0.2,\n",
    "                 emp_type_w=0.2, loc_type_w=0.2, top_n=20):\n",
    "        \"\"\"\n",
    "        Initialize the UserRecommender with scoring weights and title embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        title_embeddings : dict\n",
    "            Dictionary mapping titles/subtitles to embeddings\n",
    "        title_w : float, optional\n",
    "            Weight for title similarity, by default 0.2\n",
    "        cosine_w : float, optional\n",
    "            Weight for description similarity, by default 0.2\n",
    "        jaccard_w : float, optional\n",
    "            Weight for skill similarity, by default 0.2\n",
    "        emp_type_w : float, optional\n",
    "            Weight for employment type matching, by default 0.2\n",
    "        loc_type_w : float, optional\n",
    "            Weight for location type matching, by default 0.2\n",
    "        top_n : int, optional\n",
    "            Number of recommendations to return, by default 20\n",
    "        \"\"\"\n",
    "        self.title_weight = title_w\n",
    "        self.cosine_weight = cosine_w\n",
    "        self.jaccard_weight = jaccard_w\n",
    "        self.emp_type_weight = emp_type_w\n",
    "        self.loc_type_weight = loc_type_w\n",
    "        self.top_n = top_n\n",
    "        self.title_embeddings = title_embeddings\n",
    "\n",
    "    def compute_similarity(self, user_emb, job_emb):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between user and job skill embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_emb : numpy.ndarray\n",
    "            User's skill embedding, shape (n_features,) or (1, n_features)\n",
    "        job_emb : list or pandas.Series\n",
    "            List or Series of job embeddings, each shape (n_features,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Array of similarity scores for each job\n",
    "        \"\"\"\n",
    "        if user_emb.ndim == 1:\n",
    "            user_emb = user_emb.reshape(1, -1)\n",
    "\n",
    "        job_emb = np.vstack(job_emb)\n",
    "\n",
    "        if user_emb.shape[0] == 0 or job_emb.shape[0] == 0:\n",
    "            return np.array([0] * len(job_emb))\n",
    "\n",
    "        sim_matrix = cosine_similarity(user_emb, job_emb)\n",
    "        return sim_matrix[0]\n",
    "\n",
    "    def compute_common_skills(self, job_skills, user_skills, threshold=0.6):\n",
    "        \"\"\"\n",
    "        Count job skills that have at least one user skill with similarity > threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        job_skills : list\n",
    "            List of job skills (strings)\n",
    "        user_skills : list\n",
    "            List of user skills (strings)\n",
    "        threshold : float, optional\n",
    "            Cosine similarity threshold for a match, by default 0.6\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of common skills\n",
    "        \"\"\"\n",
    "        if not user_skills or not job_skills:\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            with open('skill_embeddings.pkl', 'rb') as f:\n",
    "                skill_to_embedding = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: skill_embeddings.pkl not found\")\n",
    "            return 0\n",
    "\n",
    "        user_embeddings = np.array([skill_to_embedding.get(skill, np.zeros(384)) for skill in user_skills])\n",
    "        job_embeddings = np.array([skill_to_embedding.get(skill, np.zeros(384)) for skill in job_skills])\n",
    "\n",
    "        sim_matrix = cosine_similarity(user_embeddings, job_embeddings)\n",
    "\n",
    "        common_skills = sum(any(sim_matrix[i, j] > threshold for i in range(len(user_skills)))\n",
    "                            for j in range(len(job_skills)))\n",
    "        return common_skills\n",
    "\n",
    "    def compute_title_similarity(self, job_title, user_subtitles):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between a job title and user subtitles using embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        job_title : str\n",
    "            Job title\n",
    "        user_subtitles : pandas.Series\n",
    "            Series of user subtitles\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Array of normalized similarity scores for each user subtitle\n",
    "        \"\"\"\n",
    "        if not isinstance(job_title, str) or not job_title.strip():\n",
    "            return np.zeros(len(user_subtitles))\n",
    "\n",
    "        job_emb = self.title_embeddings.get(job_title, np.zeros(384))\n",
    "        user_emb = np.array([self.title_embeddings.get(subtitle, np.zeros(384)) for subtitle in user_subtitles])\n",
    "\n",
    "        similarities = cosine_similarity(job_emb.reshape(1, -1), user_emb)[0]\n",
    "        max_similarity = similarities.max()\n",
    "        normalized_similarities = similarities / max_similarity if max_similarity > 0 else similarities\n",
    "        return normalized_similarities\n",
    "\n",
    "    def recommend(self, job_df, users_df):\n",
    "        \"\"\"\n",
    "        Generate user recommendations for a job.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        job_df : pandas.DataFrame\n",
    "            Job data DataFrame (single job)\n",
    "        users_df : pandas.DataFrame\n",
    "            Users data DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of recommended user dictionaries with 'id' and 'similarity_score'\n",
    "        \"\"\"\n",
    "        if job_df.empty or users_df.empty:\n",
    "            return []\n",
    "\n",
    "        users_df = users_df.copy()\n",
    "        job_df = job_df.iloc[0]\n",
    "\n",
    "        # Handle None values\n",
    "        users_df = users_df.fillna(\"\")\n",
    "        job_df = job_df.fillna(\"\")\n",
    "\n",
    "        # Calculate title similarity\n",
    "        users_df['Title Similarity'] = self.compute_title_similarity(job_df['title'], users_df['subtitle'])\n",
    "\n",
    "        # Calculate basic equality metrics\n",
    "        users_df['Employee Equal'] = users_df['employment_type'].apply(\n",
    "            lambda x: 1 if x == job_df['employee_type'] else 0)\n",
    "        users_df['Location Equal'] = users_df['location_type'].apply(\n",
    "            lambda x: 1 if x == job_df['location_type'] else 0)\n",
    "\n",
    "        # Calculate description similarity using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        job_vector = vectorizer.fit_transform([job_df['description']])\n",
    "        user_vectors = vectorizer.transform(users_df['about_me'])\n",
    "\n",
    "        raw_similarities = cosine_similarity(user_vectors, job_vector).flatten()\n",
    "        max_score = raw_similarities.max()\n",
    "        normalized_similarities = raw_similarities / max_score if max_score > 0 else raw_similarities\n",
    "        users_df['about_me_similarity'] = normalized_similarities\n",
    "\n",
    "        job_skills = set(job_df[\"skills\"])\n",
    "        users_df['common_skills'] = users_df['skills'].apply(\n",
    "            lambda user_skills: self.compute_common_skills(job_skills, user_skills, 0.5)\n",
    "        )\n",
    "\n",
    "        # Normalize skill similarity\n",
    "        max_common = users_df['common_skills'].max()\n",
    "        users_df['skill_similarity'] = users_df['common_skills'] / max_common if max_common > 0 else 0.0\n",
    "\n",
    "        # Calculate final score\n",
    "        final_scores = (\n",
    "            self.title_weight * users_df['Title Similarity'] +\n",
    "            self.cosine_weight * users_df['about_me_similarity'] +\n",
    "            self.emp_type_weight * users_df['Employee Equal'] +\n",
    "            self.loc_type_weight * users_df['Location Equal'] +\n",
    "            self.jaccard_weight * users_df['skill_similarity']\n",
    "        )\n",
    "\n",
    "        top_indices = final_scores.argsort()[::-1][:self.top_n]\n",
    "        recommendations = [\n",
    "            {\n",
    "                \"id\": int(users_df.iloc[i]['id']),\n",
    "                \"similarity_score\": float(final_scores.iloc[i])\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def recommend_for_user(job_df, users_df, title_embeddings, title_w=0.2, cosine_w=0.2, jaccard_w=0.2,\n",
    "                       emp_type_w=0.2, loc_type_w=0.2, top_n=20):\n",
    "    \"\"\"\n",
    "    Generate user recommendations for a job.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    job_df : pandas.DataFrame\n",
    "        Job data DataFrame (single job)\n",
    "    users_df : pandas.DataFrame\n",
    "        Users data DataFrame\n",
    "    title_embeddings : dict\n",
    "        Dictionary mapping titles/subtitles to embeddings\n",
    "    title_w : float, optional\n",
    "        Weight for title similarity, by default 0.2\n",
    "    cosine_w : float, optional\n",
    "        Weight for description similarity, by default 0.2\n",
    "    jaccard_w : float, optional\n",
    "        Weight for skill similarity, by default 0.2\n",
    "    emp_type_w : float, optional\n",
    "        Weight for employment type matching, by default 0.2\n",
    "    loc_type_w : float, optional\n",
    "        Weight for location type matching, by default 0.2\n",
    "    top_n : int, optional\n",
    "        Number of recommendations to return, by default 20\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of recommended user dictionaries with 'id' and 'similarity_score'\n",
    "    \"\"\"\n",
    "    if job_df.empty or users_df.empty:\n",
    "        return []\n",
    "    recommender = UserRecommender(\n",
    "        title_embeddings=title_embeddings,\n",
    "        title_w=title_w,\n",
    "        cosine_w=cosine_w,\n",
    "        jaccard_w=jaccard_w,\n",
    "        emp_type_w=emp_type_w,\n",
    "        loc_type_w=loc_type_w,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    return recommender.recommend(job_df, users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed3ec079-aeeb-4df8-9e69-a5ee18516e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded stopwords from local file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217831655a5f44ef862b130d9bc0b384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5102f150a24540989958fb05cf5d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Users: [{'id': 1238, 'similarity_score': 0.8000000029802323}, {'id': 1334, 'similarity_score': 0.7554570263624192}, {'id': 1213, 'similarity_score': 0.47545702636241916}, {'id': 1080, 'similarity_score': 0.4399408094834112}, {'id': 1124, 'similarity_score': 0.41096644210326905}, {'id': 1053, 'similarity_score': 0.39392401384818787}, {'id': 1281, 'similarity_score': 0.3830340440820479}, {'id': 1305, 'similarity_score': 0.3814196644972585}, {'id': 1154, 'similarity_score': 0.36359917163848876}, {'id': 1227, 'similarity_score': 0.35989759653806686}, {'id': 1237, 'similarity_score': 0.34757183241355644}, {'id': 1269, 'similarity_score': 0.34720982402563094}, {'id': 1187, 'similarity_score': 0.3454073944687843}, {'id': 1094, 'similarity_score': 0.3444330076956359}, {'id': 1043, 'similarity_score': 0.33730033576488494}, {'id': 1307, 'similarity_score': 0.33513206779956817}, {'id': 1200, 'similarity_score': 0.33116909668680106}, {'id': 1081, 'similarity_score': 0.3268416056036949}, {'id': 1155, 'similarity_score': 0.3253788928770629}, {'id': 1133, 'similarity_score': 0.32496920035119925}]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Main script for the job-to-user recommendation system.\n",
    "\n",
    "This script orchestrates the preprocessing of user and job data and generates\n",
    "user recommendations for a specified job.\n",
    "\"\"\"\n",
    "\n",
    "def main(input_file='test.json'):\n",
    "    \"\"\"\n",
    "    Main function to run the job-to-user recommendation system.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str, optional\n",
    "        Path to input JSON file, by default 'test.json'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of recommended user dictionaries\n",
    "    \"\"\"\n",
    "    # Preprocess data\n",
    "    users_df, job_df, skill_embeddings, title_embeddings = preprocess(input_file)\n",
    "\n",
    "    if job_df.empty or users_df.empty:\n",
    "        print(\"No valid jobs or users found for recommendation\")\n",
    "        return []\n",
    "\n",
    "    # Generate recommendations for the first job\n",
    "    recommendations = recommend_for_user(\n",
    "        job_df=job_df.iloc[[0]],  # Select first job\n",
    "        users_df=users_df,\n",
    "        title_embeddings=title_embeddings,\n",
    "        title_w=0.2,\n",
    "        cosine_w=0.2,\n",
    "        jaccard_w=0.2,\n",
    "        emp_type_w=0.2,\n",
    "        loc_type_w=0.2,\n",
    "        top_n=20\n",
    "    )\n",
    "\n",
    "    print(\"Recommended Users:\", recommendations)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc90ad-91e1-4abd-a147-eb0f0b2bb465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
